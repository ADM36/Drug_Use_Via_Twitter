{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secret_data\n",
    "import json\n",
    "import tweepy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start getting keys & secrets for running Twitter user, you will need your own user with details saved in a file named 'secret_data.txt' to run this\n",
    "CONSUMER_KEY = secret_data.CONSUMER_KEY\n",
    "CONSUMER_SECRET = secret_data.CONSUMER_SECRET\n",
    "ACCESS_TOKEN = secret_data.ACCESS_TOKEN\n",
    "ACCESS_SECRET = secret_data.ACCESS_SECRET\n",
    "#End getting keys & secrets for running Twitter user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start cache setup\n",
    "CACHE_FNAME = 'twitter_cache.json'\n",
    "try:\n",
    "    cache_file = open(CACHE_FNAME, 'r')\n",
    "    cache_contents = cache_file.read() #this is a str\n",
    "    CACHE_DICTION = json.loads(cache_contents) #this is a dict\n",
    "    cache_file.close()\n",
    "except:\n",
    "    CACHE_DICTION = {}\n",
    "#End cache setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start OAuth code\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "#End OAuth code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file containing tweet ids\n",
    "# FILENAME = 'test_tweet_ids_10.txt'\n",
    "FILENAME = 'test_tweet_ids.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start funct to grab vars from Tweet\n",
    "def get_tweet_vars(tweet):\n",
    "    try:\n",
    "        #set a number of vars for potential use\n",
    "        tweet_text = tweet['text']\n",
    "        tweet_in_reply_to_status_id_str = tweet['in_reply_to_status_id_str']\n",
    "        tweet_in_reply_to_screen_name = tweet['in_reply_to_screen_name']\n",
    "        tweet_entities_hashtags = tweet['entities']['hashtags']\n",
    "        tweet_entities_symbols = tweet['entities']['symbols']\n",
    "        tweet_entities_user_mentions = tweet['entities']['user_mentions']\n",
    "        tweet_entities_urls = tweet['entities']['urls']\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Exception in get_tweet_vars:{}\\nProblematic Tweet:{}\\n\\n'.format(e, tweet))\n",
    "#End funct to grab vars from Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start funct for cache check\n",
    "def get_tweet(found_id, label):\n",
    "    if found_id in CACHE_DICTION:\n",
    "        #if we get strange results in cache like missing child tweets then we may need to add the call to get_tweet() here\n",
    "        return CACHE_DICTION[found_id]\n",
    "#         if CACHE_DICTION[found_id]: #simple check if dict is populated or not\n",
    "#             get_tweet_vars(CACHE_DICTION[found_id])\n",
    "    else:\n",
    "        try:\n",
    "            resp = api.get_status(found_id) #resp is a class 'tweepy.models.Status'\n",
    "            json_str = json.dumps(resp._json) #json_str var is str type\n",
    "            json_obj = json.loads(json_str) #json_obj var is dict type\n",
    "        except Exception as e:\n",
    "            json_obj = {}\n",
    "            message = 'No worries. Empty entry has been made in cache.'\n",
    "            print('Exception in get_tweet:{}\\nProblematic Tweet:{}\\n\\n{}\\n'.format(e, found_id,message))\n",
    "        CACHE_DICTION[found_id] = json_obj #creating new entry in cache dict where key = 'found_id' & value = 'json_obj' which is a dict\n",
    "        CACHE_DICTION[found_id]['label'] = label\n",
    "        CACHE_DICTION[found_id]['original_related'] = 'original'\n",
    "        dumped_json_cache = json.dumps(CACHE_DICTION)\n",
    "        fw = open(CACHE_FNAME,\"w\")\n",
    "        fw.write(dumped_json_cache)\n",
    "        fw.close() # close the open file\n",
    "\n",
    "        #start recursive call to get_tweet() a.k.a. this funct if current tweet is in reply to another tweet\n",
    "        if CACHE_DICTION[found_id]['in_reply_to_status_id_str'] is not None:\n",
    "            in_reply_id = CACHE_DICTION[found_id]['in_reply_to_status_id_str']\n",
    "            get_tweet(in_reply_id) \n",
    "        #end recursive call to get_tweet()\n",
    "\n",
    "        return CACHE_DICTION[found_id]\n",
    "#         if CACHE_DICTION[found_id]: #simple check if dict is populated or not\n",
    "#             get_tweet_vars(CACHE_DICTION[found_id])\n",
    "#End funct for cache check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start funct to read in Tweet IDs from file\n",
    "def read_in_tweet_ids():\n",
    "    with open(FILENAME, 'r') as infile:\n",
    "        for line in infile:\n",
    "            found_id = line.split('\\t')[0]\n",
    "            label = line.split('\\t')[2] #label is needed for training of model\n",
    "            try:\n",
    "                get_tweet(found_id, label)\n",
    "            except:\n",
    "                pass #having exception message print here was causing duplicate messages when get_tweet failed\n",
    "#End funct to read in Tweet IDs from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = read_in_tweet_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert CACHE_DICTION to DF & transpose so it's more intuitive\n",
    "cache_df = pd.DataFrame(CACHE_DICTION).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip '\\n' char from label vals\n",
    "cache_df.label = cache_df.label.str.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only the cols we want to run through classifier & drop NaNs\n",
    "cache_df_cols = cache_df[['text','label']]\n",
    "cache_df_subset = cache_df_cols.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline SVM to process tweets\n",
    "SVM_clfr = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clfr', SVC(kernel='linear', class_weight='balanced'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: check %ages of negative & positive instances in training & testing data to consider adjusting classifier/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model to training data\n",
    "SVM_clfr = SVM_clfr.fit(cache_df_subset.text,cache_df_subset.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting against training data until we have the train/test split available\n",
    "predicted = SVM_clfr.predict(cache_df_subset.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(predicted == cache_df_subset.label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
